{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this project in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NAME = \"Paul Tluczek\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9cfe71dfae6c038c765448b792ac2426",
     "grade": false,
     "grade_id": "data_description",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Load Data\n",
    "\n",
    "The following code loads in the MNIST dataset and displays a few images and flattens the images and sets some autograder variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "53ebeabcec0d8358bcddc343f63e2bee",
     "grade": false,
     "grade_id": "data_code",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy\n/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/H\ntn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+\n/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/f\nv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y3\n5wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FE6FE263FD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/0lEQVR4nGNgGHhgPP/vfCMccgbv\n/vz58xa7nNnjv3/ev/xjyYYpxWXz4M/fP6dC/vytgggwIUnOPCDDwMBgxHOQQRdD0tibkfFQKeOL\n85OYGLG5ZTOPd6UoA8Pfz2gOVlv69+WFEAj775+lKHLsm/58cBeWgUkeRpG0/PPHHs5Blzz2dx+C\n8//vEWTX+hj834SQ/Pf/ArLG0D/PJOHWt//dxYMqeR8u1/znoTsDquREKMtg6Z+1DKgg7O9DCKPo\n3d9FaHIMoX9+TjKQDd308O/95RaYkn/+PL3+58+fI03oUgwMMsf//Pn758/LiZhSDAwMkg1//v7p\nVcUqR1cAAKxwbkTVIzd2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FE6FAD49358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nGNgGArA+YU6AwMDAwMT\nAwMDg10gqqTpGQaEpEMQihyTohwjgndnMYqk9L9FSDqZUE2dw3AbIaknjirJz7AbIenFiSInrsjw\nFCGpznAVWbJH/NZnCIuFgYGBgeE0XIbPI8aNofkDsqQQAwODPpOzDFs00/eTP1nOQlUyMjAwTEv/\n8IiBQY/xz7drJ88cfPlEkI0BoTProRUDA8OjjddOMDAwMKSJ3mPACVb+64QxmbBIb8AnyYBHklEV\nj+R/JjySDJb4jMVj5/b/OB1IJQAAg3ksR3QPgSAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FE6FAD493C8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnElEQVR4nGNgGPyg5u9/e1xyCV9+\n/7WDMJkwJOXZcRvq8ub3ZXkO7HI2T37/jsOlcfbfv3txyYn8/f3aCYecwtm/v+twacz4/XcHPw65\ngA+/D4rjMvTv37/zcRk6/ffv3+o45Azu/v69BpfGV79/H+HBJfn39+9IXHLz///9K4/Lxid/v/fg\nCHAGh99/76CLYcYnNskbx/ApoyoAAGeYO0QsY6cRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FE6FAD49438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nN3QPwtBYRQG8EMU0e0u\nZLIw+QKXRZlMGC0GX8CglE0pk0VxPwQmE5YrJYPVIjYMlImSwXNiMOi97319AM/6O6fzh+g/Y5hr\n5mrRNByseAZba4D7EnlSN8wy3uAYXJOwDEw0ohKwD9mtxehqRLQBCnZr8GPkJ/Ll79y0m37GiIji\nK2AQsGMYiIbryyvjmZO20U9gAIcjTg43GhfethOROToO+En6xRUlZhnSjd+I6BY7xVIRY79w4Xap\nR9IOSTWWYSWUqE0xlH771R7UrULefm5U2pxVCt0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FE6FAD493C8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import array_to_img, img_to_array\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "[X, y], _ = mnist.load_data()\n",
    "for x in X[:5]:\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    img = array_to_img(x)\n",
    "    display(img)\n",
    "\n",
    "X = X.reshape([60_000, 28*28]) / 255.\n",
    "Y = to_categorical(y)\n",
    "X, Y = X[:50], Y[:50]\n",
    "\n",
    "M, N = X.shape\n",
    "C = np.unique(y).shape[0]\n",
    "H = 16\n",
    "\n",
    "def passed(): print('✅')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "341d578656e001b360d23c62630c3924",
     "grade": false,
     "grade_id": "layers_description",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Task\n",
    "\n",
    "- Implement `Dense`, `Sigmoid`, and `SoftmaxCE` layers as classes and stick them in a file `layers.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1a91b517e29663f52841c07a4624f507",
     "grade": false,
     "grade_id": "layers_solution",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile layers.py\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "class Dense:\n",
    "    def __init__(self, N, H):\n",
    "        self.W = np.random.randn(N, H)\n",
    "        self.b = np.random.randn(H)\n",
    "    def forward(self, X):\n",
    "        Z = np.dot(X,self.W) + self.b\n",
    "        self.X = X\n",
    "        self.cache = locals()             \n",
    "        return Z\n",
    "    def backward(self, dZ):\n",
    "        X = self.cache['X']        \n",
    "        db = dZ.sum(axis=0)\n",
    "        dX, dW = np.dot(dZ,np.transpose(self.W)), np.dot(np.transpose(X),dZ)\n",
    "        return dX, dW, db\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, Z):\n",
    "        H = 1 / (1 + np.exp(-Z))\n",
    "        self.cache = locals()        \n",
    "        return H\n",
    "    def backward(self, dH):\n",
    "        H = self.cache['H']        \n",
    "        dZ = H * (1-H) * dH\n",
    "        return dZ\n",
    "    \n",
    "class SoftmaxCE:\n",
    "    def forward(self, S, Y):\n",
    "        P = np.exp(S) / (np.exp(S).sum(axis=1, keepdims=True))\n",
    "        y = Y.argmax(axis=1)\n",
    "        M = len(P)\n",
    "        L = P[np.arange(M), y]\n",
    "        L = -np.log(L)\n",
    "        L = np.expand_dims(L, axis=-1)\n",
    "        self.cache = locals()\n",
    "        return L\n",
    "    def backward(self, dL):\n",
    "        P, y, M = self.cache['P'], self.cache['y'], self.cache['M']\n",
    "        dLdS = P\n",
    "        dLdS[np.arange(M), y] -= 1\n",
    "        dS = dLdS * dL\n",
    "        return dS  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1ad9f8e29d7066dbb22d7aacc958d816",
     "grade": false,
     "grade_id": "mlp_description",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Task\n",
    "\n",
    "- Define a one-hidden layer perceptron class called `LayeredMLP` which uses Dense, Sigmoid, and SoftmaxCE layers as in the computational graph\n",
    "\n",
    "![](images/mlp_predict.svg)\n",
    "\n",
    "where\n",
    "\n",
    "- $\\mathbf{X} \\in \\mathbb{R}^{M \\times N}$\n",
    "- $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{N \\times H}$ and $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{H}$\n",
    "- $\\mathbf{Z} \\in \\mathbb{R}^{M \\times H}$\n",
    "- $\\mathbf{H} \\in \\mathbb{R}^{M \\times H}$\n",
    "- $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{H \\times C}$ and $\\mathbf{b}^{(2)} \\in \\mathbb{R}^{C}$\n",
    "- $\\mathbf{S} \\in \\mathbb{R}^{M \\times C}$\n",
    "- $\\mathbf{Y} \\in \\mathbb{R}^{M \\times C}$\n",
    "- $\\mathbf{L} \\in \\mathbb{R}^{M}$ and $\\overline{\\ell} \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c21ec5e306a4fb8927058eb135e44030",
     "grade": false,
     "grade_id": "mlp_solution",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting classifiers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile classifiers.py\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "import layers\n",
    "       \n",
    "class LayeredMLP():\n",
    "    def __init__(self, nb_feature, nb_hidden, nb_class):        \n",
    "        self.dense1 = layers.Dense(N=nb_feature, H=nb_hidden)\n",
    "        self.sigmoid = layers.Sigmoid()\n",
    "        self.dense2 = layers.Dense(N=nb_hidden, H=nb_class)\n",
    "        self.softmaxce = layers.SoftmaxCE()\n",
    "        \n",
    "    def _f(self, X):\n",
    "        Z = self.dense1.forward(X) \n",
    "        H = self.sigmoid.forward(Z)\n",
    "        S = self.dense2.forward(H)        \n",
    "        self.cache = locals()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        self._f(X)\n",
    "        S = self.cache['S']\n",
    "        P = np.exp(S) / np.exp(S).sum(axis=1, keepdims=True)\n",
    "        return P\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        return np.mean((self.predict(X)).argmax(axis=1) == Y.argmax(axis=1))  \n",
    "    \n",
    "    def forward(self, X, Y):\n",
    "        self._f(X)\n",
    "        S = self.cache['S']\n",
    "        L = self.softmaxce.forward(S, Y)        \n",
    "        return np.mean(L)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "10ffcfb7827eaa3b97abe2ec0de0a71b",
     "grade": false,
     "grade_id": "constructor_blurb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Constructor Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e6456fc2c088b29b2734146de940878a",
     "grade": true,
     "grade_id": "constructor_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from classifiers import LayeredMLP\n",
    "import layers\n",
    "\n",
    "mlp = LayeredMLP(nb_feature=N, nb_hidden=H, nb_class=C)\n",
    "\n",
    "assert type(mlp.dense1) == layers.Dense\n",
    "assert hasattr(mlp.dense1, 'W')\n",
    "assert hasattr(mlp.dense1, 'b')\n",
    "assert type(mlp.sigmoid) == layers.Sigmoid\n",
    "assert type(mlp.dense2) == layers.Dense\n",
    "assert hasattr(mlp.dense2, 'W')\n",
    "assert hasattr(mlp.dense2, 'b')\n",
    "assert type(mlp.softmaxce) == layers.SoftmaxCE\n",
    "\n",
    "passed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "66b654e20e83c25cbc448a69673cf198",
     "grade": false,
     "grade_id": "predict_blurb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Prediction Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79ccc7b5bc7b4b2854e54468976bd4fd",
     "grade": true,
     "grade_id": "prediction_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "from classifiers import LayeredMLP\n",
    "\n",
    "mlp = LayeredMLP(nb_feature=N, nb_hidden=H, nb_class=C)\n",
    "\n",
    "S = mlp.predict(X)\n",
    "nb_train_ = len(X)\n",
    "assert S.shape == (M, C)\n",
    "\n",
    "passed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2163f3af2dc3c7f00eab3beacec44034",
     "grade": false,
     "grade_id": "evaluation_blurb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Evaluation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7fcf89e44691cc461e79eccb073d677e",
     "grade": true,
     "grade_id": "evaluation_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "from classifiers import LayeredMLP\n",
    "\n",
    "mlp = LayeredMLP(nb_feature=N, nb_hidden=H, nb_class=C)\n",
    "\n",
    "acc = mlp.evaluate(X, Y)\n",
    "assert type(acc) == np.float64\n",
    "assert 0 <= acc <= 1\n",
    "\n",
    "passed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e834ebdb01780cdc281380d484305035",
     "grade": false,
     "grade_id": "gradients_description",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Task\n",
    "\n",
    "- Implement a `LayeredMLPWithGDOptimizer` class which performs optimization via gradient descent and extends your `LayeredMLP` class\n",
    "\n",
    "# Requirement\n",
    "\n",
    "- You must use backpropagation to compute gradients. To demonstrate this I am requiring your `_get_gradients()` function needs to return the gradient of every intermediate value in the computational graph as in\n",
    "\n",
    "![](images/mlp_full.svg)\n",
    "\n",
    "including `dX` (not pictured). You don't have to return `dloss`. Check the tests below to clear up any confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "726c59a0d5c5696a1f9177b4b04f8367",
     "grade": false,
     "grade_id": "gradients_solution",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to classifiers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a classifiers.py\n",
    "\n",
    "# YOUR CODE HERE\n",
    "class LayeredMLPWithGDOptimizer(LayeredMLP):\n",
    "    def __init__(self, nb_feature, nb_hidden, nb_class, alphalr=0.01):        \n",
    "        LayeredMLP.__init__(self, nb_feature, nb_hidden, nb_class) \n",
    "        self.alphalr = alphalr\n",
    " \n",
    "    def fit(self, X, Y, nb_epoch=1):\n",
    "        for _ in range(nb_epoch):\n",
    "            # Store current parameters\n",
    "            curr_W1, curr_b1, curr_W2, curr_b2 = self.dense1.W, self.dense1.b, self.dense2.W, self.dense2.b           \n",
    "            # Store current parameter loss\n",
    "            curr_loss = self.forward(X, Y) \n",
    " \n",
    "            dL = np.ones([len(X), 1], dtype='float64')\n",
    "            dS = self.softmaxce.backward(dL)    \n",
    "            dH, dW2, db2 = self.dense2.backward(dS)\n",
    "            dZ = self.sigmoid.backward(dH)\n",
    "            dX, dW1, db1 = self.dense1.backward(dZ)\n",
    "            \n",
    "            # New parameters after GD\n",
    "            self.dense1.W = self.dense1.W - (self.alphalr * dW1)\n",
    "            self.dense1.b = self.dense1.b - (self.alphalr * db1)\n",
    "            self.dense2.W = self.dense2.W - (self.alphalr * dW2)\n",
    "            self.dense2.b = self.dense2.b - (self.alphalr * db2)\n",
    "            \n",
    "            new_loss = self.forward(X, Y)\n",
    "\n",
    "            # If new parameters give higher loss, revert to old parameters \n",
    "            if curr_loss < new_loss:                               \n",
    "                self.dense1.W, self.dense1.b, self.dense2.W, self.dense2.b = curr_W1, curr_b1, curr_W2, curr_b2  \n",
    "    \n",
    "    def _get_gradients(self, X, Y):\n",
    "        curr_loss = self.forward(X, Y) \n",
    "        dL = np.ones([len(X), 1], dtype='float64')\n",
    "        dS = self.softmaxce.backward(dL)    \n",
    "        dH, dW2, db2 = self.dense2.backward(dS)\n",
    "        dZ = self.sigmoid.backward(dH)\n",
    "        dX, dW1, db1 = self.dense1.backward(dZ)\n",
    "        return dX, dW1, db1, dZ, dH, dW2, db2, dS                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "10d75c5b84146ba3896047ce95c44908",
     "grade": false,
     "grade_id": "gradients_blurb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Gradient Checking Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0fef657d4e033be30a4739f2fb877bb4",
     "grade": true,
     "grade_id": "gradients_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from classifiers import LayeredMLP, LayeredMLPWithGDOptimizer\n",
    "\n",
    "mlp = LayeredMLPWithGDOptimizer(nb_feature=N, nb_hidden=H, nb_class=C)\n",
    "assert issubclass(LayeredMLPWithGDOptimizer, LayeredMLP)\n",
    "\n",
    "gradients = mlp._get_gradients(X, Y)\n",
    "for gradient in gradients:\n",
    "    assert type(gradient) == np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "50eb25f31f236027bd6ff4810a4f80b0",
     "grade": false,
     "grade_id": "gradient_blurb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Gradient Checking Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5aca1f661354de17ee0baed52ea8c2a4",
     "grade": true,
     "grade_id": "gradient_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:dW1 check failed with a difference of 0.06827340895730426!\n",
      "WARNING:root:db1 check failed with a difference of 1.1178841746240136!\n",
      "WARNING:root:dW2 check failed with a difference of 28.91086438689801!\n",
      "WARNING:root:db2 check failed with a difference of 75.62668312523859!\n"
     ]
    }
   ],
   "source": [
    "from classifiers import LayeredMLPWithGDOptimizer\n",
    "from checking import estimate_gradients\n",
    "\n",
    "mlp = LayeredMLPWithGDOptimizer(nb_feature=N, nb_hidden=H, nb_class=C)\n",
    "\n",
    "estimated_gradients = estimate_gradients(mlp, X, Y)\n",
    "dX, dW1, db1, dZ, dH, dW2, db2, dS = mlp._get_gradients(X, Y)\n",
    "analytical_gradients, params = [dW1, db1, dW2, db2], ['dW1', 'db1', 'dW2', 'db2']\n",
    "grad_pairs = zip(estimated_gradients, analytical_gradients, params)\n",
    "for i, (estimated_gradient, analytic_gradient, param) in enumerate(grad_pairs):\n",
    "    try:\n",
    "        assert np.allclose(estimated_gradient, analytic_gradient)\n",
    "    except:\n",
    "        norm = np.square(estimated_gradient - analytic_gradient).mean()\n",
    "        logging.warning(f'{param} check failed with a difference of {norm}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "73e0cb7f4c79e43926263c3e53066d5a",
     "grade": false,
     "grade_id": "optimizer_blurb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Gradient Descent Optimizer Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6007fda16d797614ace0436ecd0967fe",
     "grade": true,
     "grade_id": "optimizer_tests",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "from classifiers import LayeredMLPWithGDOptimizer\n",
    "\n",
    "mlp = LayeredMLPWithGDOptimizer(nb_feature=N, nb_hidden=H, nb_class=C)\n",
    "\n",
    "X_sample, Y_sample = X[:50], Y[:50]\n",
    "acc = mlp.evaluate(X_sample, Y_sample)\n",
    "loss = mlp.forward(X_sample, Y_sample)\n",
    "for _ in range(10):\n",
    "    mlp.fit(X_sample, Y_sample, nb_epoch=10)\n",
    "    assert mlp.forward(X_sample, Y_sample) < loss\n",
    "    loss = mlp.forward(X_sample, Y_sample)\n",
    "    \n",
    "assert mlp.evaluate(X_sample, Y_sample) > acc\n",
    "\n",
    "passed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6a2daeba672f2bbee33bfe3796ad3fd0",
     "grade": false,
     "grade_id": "ignore",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Ignore Cell Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "7d8afec2c4454c9be1c1455cb533af35",
     "grade": false,
     "grade_id": "ignore_solution",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
